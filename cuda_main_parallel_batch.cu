//
// Created by giacomo on 1/27/26.
//



#include <iostream>
#include <string>
#include <filesystem>
#include <fstream>

namespace fs = std::filesystem;

#include "opencv2/core.hpp"
#include <opencv2/core/mat.hpp>
#include <opencv2/imgcodecs.hpp>
#include <cuda_runtime.h>
#include <cuda_runtime_api.h>
#include "src/kernel_functions.h"
#include "src/cuda_kernel.cuh"
#include <omp.h>

//Generated by Gemini pro to handle CUDA errors
#define CUDA_CHECK(call) \
do { \
    cudaError_t err = call; \
    if (err != cudaSuccess) { \
        fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \n", \
        __FILE__, __LINE__, err, cudaGetErrorString(err)); \
        exit(EXIT_FAILURE); \
    } \
} while (0)


//Based on my GTX 1070 with 8GB VRAM
#define BYTE_BATCH_SIZE 3145728000

int main(int argc, char* argv[]) {
    Config cfg;

    cfg.parse(argc, argv);

    omp_set_num_threads(cfg.threads);

    auto start_e2e = std::chrono::high_resolution_clock::now();

    float kernel[MAX_K*MAX_K];
    generateKernel(kernel, cfg.kernelType);

    CUDA_CHECK(loadKernel(kernel, cfg.K));

    //we choose blocks 32x4. So, to cover all the images 150*150 we need a grid 5x38
    //for 1024x1024 we choose blocks 32x4, so a grid 32x256

    dim3 dimBlock(32,4);

    std::string path = cfg.datasetPath;



    std::vector<std::filesystem::path> imgList;
    for (const auto & entry : fs::directory_iterator(path)) {
        imgList.push_back(entry.path());
    }

    cv::Mat firstImg = cv::imread(imgList[0], cv::IMREAD_UNCHANGED);
    cv::Size size = firstImg.size();;
    int imgType = firstImg.type();
    int channels = firstImg.channels();
    
    const int W = size.width;
    const int H = size.height;
    

    const size_t BATCH_SIZE = BYTE_BATCH_SIZE/(W*H*channels);
    unsigned char* batchPtr;
    unsigned char* deviceInput;
    unsigned char* deviceOutput;

    CUDA_CHECK(cudaMallocHost(&batchPtr, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));
    CUDA_CHECK(cudaMalloc(&deviceInput, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));
    CUDA_CHECK(cudaMalloc(&deviceOutput, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));

    const int  threadBatchSize = BATCH_SIZE/cfg.threads;
    cudaStream_t streams[cfg.threads];

    for (int i = 0; i < cfg.threads; i++) {

        CUDA_CHECK(cudaStreamCreate(&streams[i]));
    }


#pragma omp parallel for default(none) shared(imgList, batchPtr, threadBatchSize, dimBlock, streams, deviceInput, deviceOutput, W, H, size, imgType, channels, cfg) firstprivate(stderr)
    for (int i=0; i <= imgList.size()/threadBatchSize; i++) {

        int currentBatchSize=0;
        int id = omp_get_thread_num();

        for (long long j=0; j<threadBatchSize; j++) {

            int generalIndex= i*threadBatchSize + j;

            if (generalIndex >= imgList.size()) {
                continue ;
            }

            currentBatchSize++;
            cv::Mat inputImg = cv::imread(imgList[generalIndex], cv::IMREAD_UNCHANGED);
            memcpy(batchPtr + (j+threadBatchSize*id)*sizeof(unsigned char)*W*H*channels, inputImg.ptr(), sizeof(unsigned char)*size.area()*channels);
        }
        size_t ptrOffset = threadBatchSize*id*sizeof(unsigned char)*W*H*channels;
        dim3 dimGrid((size.width/dimBlock.x)+1,(size.height/dimBlock.y)+1, currentBatchSize);
        CUDA_CHECK(cudaMemcpyAsync(deviceInput + ptrOffset , batchPtr + ptrOffset, sizeof(unsigned char)*W*H*channels*currentBatchSize, cudaMemcpyHostToDevice, streams[id]));
        applyCudaKernel <<<dimGrid, dimBlock, 0, streams[id]>>> (deviceInput + ptrOffset, deviceOutput + ptrOffset, cfg.K, size.width, size.height, channels);
        CUDA_CHECK(cudaMemcpyAsync(batchPtr + ptrOffset, deviceOutput + ptrOffset, sizeof(unsigned char)*W*H*channels*currentBatchSize, cudaMemcpyDeviceToHost, streams[id]));
        CUDA_CHECK(cudaStreamSynchronize(streams[id]));

        for (long long j=0; j<currentBatchSize; j++) {

            int generalIndex= i*threadBatchSize + j;

            if (generalIndex >= imgList.size()) {
                continue;
            }

            cv::Mat outputImg =  cv::Mat::zeros(size, imgType);
            memcpy(outputImg.ptr(), batchPtr + (j+threadBatchSize*id)*sizeof(unsigned char)*W*H*channels, sizeof(unsigned char)*size.area()*channels);

            std::string outputPath = "../cuda_output_" + std::to_string(size.width) + "_" + "k=" + std::to_string(cfg.K) + "/" + imgList[generalIndex].filename().string();
            cv::imwrite(outputPath, outputImg);

        }

    }

    for (int i = 0; i < cfg.threads; i++) {
        CUDA_CHECK(cudaStreamDestroy(streams[i]));
    }

    CUDA_CHECK(cudaFree(deviceInput));
    CUDA_CHECK(cudaFree(deviceOutput));
    CUDA_CHECK(cudaFreeHost(batchPtr));

    auto end_e2e = std::chrono::high_resolution_clock::now();
    auto time_e2e = std::chrono::duration_cast<std::chrono::duration<double>>(end_e2e - start_e2e).count();

    std::cout << "Time Taken End to End:" << time_e2e << "s" << std::endl;
    std::cout << "Elements Elaborated:" << imgList.size() << std::endl;

    append_csv(cfg, size.width, imgList.size(), 0, time_e2e, cfg.outputPath);
}
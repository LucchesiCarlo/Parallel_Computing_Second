//
// Created by giacomo on 1/26/26.
//


#include <iostream>
#include <string>
#include <filesystem>
#include <fstream>

namespace fs = std::filesystem;

#include "opencv2/core.hpp"
#include <opencv2/core/mat.hpp>
#include <opencv2/imgcodecs.hpp>
#include <cuda_runtime.h>
#include <cuda_runtime_api.h>
#include "src/kernel_functions.h"
#include "src/cuda_kernel.cuh"
#include <omp.h>

//Generated by Gemini pro to handle CUDA errors
#define CUDA_CHECK(call) \
do { \
    cudaError_t err = call; \
    if (err != cudaSuccess) { \
        fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \n", \
        __FILE__, __LINE__, err, cudaGetErrorString(err)); \
        exit(EXIT_FAILURE); \
    } \
} while (0)


//Based on my GTX 1070 with 8GB VRAM
#define BYTE_BATCH_SIZE 3145728000

int main(int argc, char* argv[]) {
    Config cfg;

    cfg.parse(argc, argv);

    omp_set_num_threads(cfg.threads);

    auto start_e2e = std::chrono::high_resolution_clock::now();

    float kernel[MAX_K*MAX_K];
    generateKernel(kernel, cfg.kernelType);

    CUDA_CHECK(loadKernel(kernel, cfg.K));

    //we choose blocks 32x4. So, to cover all the images 150*150 we need a grid 5x38
    //for 1024x1024 we choose blocks 32x4, so a grid 32x256

    dim3 dimBlock(32,4);

    std::string path = cfg.datasetPath;

    std::vector<std::filesystem::path> imgList;
    for (const auto & entry : fs::directory_iterator(path)) {
        imgList.push_back(entry.path());
    }
    cv::Mat firstImg = cv::imread(imgList[0], cv::IMREAD_UNCHANGED);
    cv::Size size = firstImg.size();;
    int imgType = firstImg.type();
    int channels = firstImg.channels();
    
    const int W = size.width;
    const int H = size.height;
    

    const size_t BATCH_SIZE = BYTE_BATCH_SIZE/(W*H*channels);
    unsigned char* batchPtr;
    unsigned char* deviceInput;
    unsigned char* deviceOutput;

    CUDA_CHECK(cudaMallocHost(&batchPtr, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));
    CUDA_CHECK(cudaMalloc(&deviceInput, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));
    CUDA_CHECK(cudaMalloc(&deviceOutput, sizeof(unsigned char)*W*H*channels*BATCH_SIZE));


    for (int i=0; i <= imgList.size()/BATCH_SIZE; i++) {

        int currentBatchSize=0;

#pragma omp parallel for default(none) shared(imgList, currentBatchSize, i, batchPtr, W, H, size, imgType, channels, BATCH_SIZE)

        for (long long j=0; j<BATCH_SIZE; j++) {

            int generalIndex= i*BATCH_SIZE + j;

            if (generalIndex >= imgList.size()) {
                continue ;
            }

#pragma omp atomic
            currentBatchSize++;
            cv::Mat inputImg = cv::imread(imgList[generalIndex], cv::IMREAD_UNCHANGED);
            memcpy(batchPtr + j*sizeof(unsigned char)*W*H*channels, inputImg.ptr(), sizeof(unsigned char)*size.area()*channels);
        }

        dim3 dimGrid((size.width/dimBlock.x)+1,(size.height/dimBlock.y)+1, currentBatchSize);
        CUDA_CHECK(cudaMemcpy(deviceInput, batchPtr, sizeof(unsigned char)*W*H*channels*BATCH_SIZE, cudaMemcpyHostToDevice));
        applyCudaKernel <<<dimGrid, dimBlock>>> (deviceInput, deviceOutput, cfg.K, size.width, size.height, channels);
        CUDA_CHECK(cudaMemcpy(batchPtr, deviceOutput, sizeof(unsigned char)*W*H*channels*BATCH_SIZE, cudaMemcpyDeviceToHost));

#pragma omp parallel for default(none) shared(imgList, currentBatchSize, size, imgType, channels, i, cfg, batchPtr,W,H, BATCH_SIZE)
        for (long long j=0; j<currentBatchSize; j++) {

            int generalIndex= i*BATCH_SIZE + j;

            if (generalIndex >= imgList.size()) {
                continue;
            }

            cv::Mat outputImg =  cv::Mat::zeros(size, imgType);
            memcpy(outputImg.ptr(), batchPtr + j*sizeof(unsigned char)*W*H*channels, sizeof(unsigned char)*size.area()*channels);

            std::string outputPath = "../cuda_output_" + std::to_string(size.width) + "_" + "k=" + std::to_string(cfg.K) + "/" + imgList[generalIndex].filename().string();
            cv::imwrite(outputPath, outputImg);

        }

    }
        CUDA_CHECK(cudaFree(deviceInput));
        CUDA_CHECK(cudaFree(deviceOutput));
        CUDA_CHECK(cudaFreeHost(batchPtr));

    auto end_e2e = std::chrono::high_resolution_clock::now();
    auto time_e2e = std::chrono::duration_cast<std::chrono::duration<double>>(end_e2e - start_e2e).count();

    std::cout << "Time Taken End to End:" << time_e2e << "s" << std::endl;
    std::cout << "Elements Elaborated:" << imgList.size() << std::endl;

    append_csv(cfg, size.width, imgList.size(), 0, time_e2e, cfg.outputPath);
}